{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82127e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae53604a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your JSONL file\n",
    "file_path = \"final_data_reindexed.jsonl\"\n",
    "\n",
    "# List to store the number of exchanges per dialogue\n",
    "exchange_counts = []\n",
    "dialogues = []\n",
    "# Read JSONL file and count exchanges\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        dialogue = json.loads(line)\n",
    "        num_exchanges = len(dialogue.get(\"original_context\", []))\n",
    "        exchange_counts.append(num_exchanges/2)\n",
    "\n",
    "# Count frequency of each exchange count\n",
    "count_distribution = Counter(exchange_counts[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9a2b892",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({3.0: 15,\n",
       "         6.0: 11,\n",
       "         7.0: 9,\n",
       "         1.0: 9,\n",
       "         4.0: 8,\n",
       "         5.0: 4,\n",
       "         8.0: 2,\n",
       "         2.0: 1,\n",
       "         10.0: 1})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "203aefe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_path = \"llama2_generated_responses.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8181740",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Plot histogram\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# plt.bar(count_distribution.keys(), count_distribution.values(), width=0.6)\n",
    "# plt.xlabel(\"Number of Exchanges in Original Context\")\n",
    "# plt.ylabel(\"Number of Dialogues\")\n",
    "# plt.title(\"Histogram of Exchange Counts in Dialogues\")\n",
    "# plt.xticks(sorted(count_distribution.keys()))\n",
    "# plt.grid(axis=\"y\")\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c280fe31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09703d899c3e4f589bdcb2ba010a6543",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model\n",
    "model_name = \"google/flan-t5-xl\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a29a0948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(datapoint, max_new_tokens=200):\n",
    "    dialogue_str = \"\\n\".join([f\"{turn['speaker']}: {turn['text']}\" for turn in datapoint['original_context']])\n",
    "\n",
    "    # prompt = (\n",
    "    #     # f\"Assume this was our past conversation:\\n{dialogue_str}\\n\\n\"\n",
    "    #     # f\"You are the 'System' and I am the 'User'. Now here is my next query right after the past conversation:\\n\"\n",
    "    #     # f\"{datapoint['user_revision']['text']}\\n\\nSystem: Please respond with a full step-by-step answer tailored to the revised context.\"\n",
    "    #     f\"Here is our past conversation:\\n{dialogue_str}\\n\\n\"\n",
    "    #     f\"You are the 'System' and I am the 'User'. Now here is my next message in the conversation:\\n\"\n",
    "    #     f\"{datapoint['user_revision']['text']}\\n\\n\"\n",
    "    #     f\"please answer this user query, if you are not sure what to answer, continue the conversation with this user query, don't give answer of 'System:' or 'User:' \\n\\n\"\n",
    "    # )\n",
    "    prompt = (\n",
    "    f\"You are a helpful system continuing a conversation with a user.\\n\\n\"\n",
    "    f\"Here is the prior dialogue:\\n{dialogue_str}\\n\\n\"\n",
    "    f\"The user now says:\\n\\\"{datapoint['user_revision']['text']}\\\"\\n\\n\"\n",
    "    f\"Please respond meaningfully to the user's new message, based on the earlier context. \"\n",
    "    f\"Write only your reply—do not prefix with 'System:' or 'User:'.\"\n",
    ")\n",
    "    #print(prompt)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b075ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = \"xxx\"\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=api_key)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f2024c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    sample_block = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ff5687",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logic_extraction(response):\n",
    "   print(\"checker1\")\n",
    "   prompt =  f\"\"\"\n",
    "   Given the following system response, extract the underlying logical points in a list format. Each point should reflect a distinct idea or reasoning step present in the response.\n",
    "\n",
    "   System Response:\n",
    "   {response}\n",
    "   Output the logic points in a bullet list format. Each item should be a standalone logical statement.\n",
    "   \"\"\"\n",
    "\n",
    "   # Call OpenAI API using the updated format\n",
    "   completion = client.chat.completions.create(\n",
    "      model=\"gpt-4o\",\n",
    "      messages=[\n",
    "         {\"role\": \"system\", \"content\": (\n",
    "               \"You are a dialogue evaluation assistant specializing in extracting logical reasoning from system responses. Focus on capturing distinct logical points in list form.\"\n",
    "         )},\n",
    "         {\"role\": \"user\", \"content\": prompt}\n",
    "      ]\n",
    "   )\n",
    "   \n",
    "   # Extract and print response\n",
    "   result = completion.choices[0].message.content\n",
    "   print(f\"extracted logic points: {result}\\n\")\n",
    "   return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1f8b3ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_t5_response(datapoint, t5_response):\n",
    "    print(\"checker2\")\n",
    "    dialogue_str = \"\\n\".join([f\"{turn['speaker']}: {turn['text']}\" for turn in datapoint['original_context']])\n",
    "    evaluation_prompt = f\"\"\"\n",
    "    You are evaluating how well a generated system response adapts to a user's revised query and revised logic chain in the context of a past conversation.\n",
    "\n",
    "    Here is the original conversation:\n",
    "    {dialogue_str}\n",
    "\n",
    "    Here is the user's revision:\n",
    "    {datapoint['user_revision']['text']}\n",
    "\n",
    "    Here is the T5-generated system response:\n",
    "    {t5_response}\n",
    "\n",
    "    Here is the T5-generated system response's logic points:\n",
    "    {logic_extraction(t5_response)}\n",
    "    \n",
    "    Here is the original conversation's logic points:\n",
    "    {datapoint['logical_shift']['original_logic']}\n",
    "\n",
    "    Here is the sample correct revised system response:\n",
    "    {datapoint['correct_revised_response']['text']}\n",
    "\n",
    "    Here is the sample correct revised system response's logic points:\n",
    "    {datapoint['logical_shift']['correct_revised_logic']}\n",
    "\n",
    "    Here is the sample wrong revised system response:\n",
    "    {datapoint['wrong_revised_response']['text']}\n",
    "\n",
    "    Here is the sample wrong revised system response's logic points:\n",
    "    {datapoint['logical_shift']['wrong_revised_logic']}\n",
    "\n",
    "    Your task: Using the dialogues and sample correct/wrong answers as reference, evalute and determine whether the logic chain of T5-generated response adapts to the user's revision and the new logic based on such revision. Does it reflect the updated logic properly? Is it closer to the correct or incorrect version? The common wrong answer is either failing to update or using updated infomariotn in initial logic chain (causing contradiction).\n",
    "\n",
    "    Return only a decimal score between 0 and 1. The score is:\n",
    "    (number of aligned logic points) / (total number of extracted logic points). Then briefly justify your score in 1–2 sentences.\n",
    "\n",
    "    Your task: Using the dialogue and the sample correct and incorrect responses as reference, evaluate whether the logic chain in the T5-generated system response adapts to the user's revised query appropriately. \n",
    "\n",
    "    A correct response should update the logic based on the new user context without contradictions. Common errors include failing to update the logic or inserting the revised information into the original logic without adjustment, which leads to inconsistencies.\n",
    "    You can use {sample_block} as an example for evalution.\n",
    "\n",
    "    Return only a decimal score between 0 and 1, representing:\n",
    "    (number of logically aligned points) / (total number of extracted logic points)\n",
    "\n",
    "    If the T5-generated system response has no meaning like just \"System:\", return -1.\n",
    "    Then briefly justify your score in 1–2 sentences.\n",
    "    \"\"\"\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": (\n",
    "                \"You are an evaluation assistant trained to assess whether a generated dialogue response reflects a logically correct update based on a user's modified query.\"\n",
    "            )},\n",
    "            {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result = completion.choices[0].message.content.strip()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9260236a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_score_via_gpt(evaluation_output, client, model=\"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    Uses GPT to extract only the decimal score from a full evaluation output.\n",
    "    \n",
    "    Args:\n",
    "        evaluation_output (str): The full evaluation text (score + explanation).\n",
    "        client: OpenAI client instance.\n",
    "        model (str): The OpenAI model name.\n",
    "        \n",
    "    Returns:\n",
    "        str: The score string, like \"0.25\"\n",
    "    \"\"\"\n",
    "    system_prompt = \"You are an assistant that extracts evaluation scores from the evaluation analysis.\"\n",
    "    user_prompt = f\"\"\"Here is the evaluation output:\n",
    "---\n",
    "{evaluation_output}\n",
    "---\n",
    "\n",
    "Your task is to extract and return ONLY the numeric score from the text above. \n",
    "The score is a decimal between 0 and 1. Do not return anything else—no punctuation, no explanation, just the number.\n",
    "\"\"\"\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a938b46",
   "metadata": {},
   "source": [
    "evalution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b8ed607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"final_data_reindexed.jsonl\"\n",
    "data = []\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))\n",
    "\n",
    "\n",
    "# temp_path = \"llama2_generated_responses.jsonl\"\n",
    "# temp_data = []\n",
    "# with open(temp_path, \"r\", encoding=\"utf-8\") as f:\n",
    "#     for line in f:\n",
    "#         temp_data.append(json.loads(line))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62fe86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = []\n",
    "# responses = []\n",
    "# for i, datapoint in enumerate(data[2:]):\n",
    "#     print(f\"Processing datapoint {i+1}/{len(data[2:])}\")\n",
    "#     t5_response = generate_response(datapoint)\n",
    "#     responses.append(t5_response)\n",
    "#     evaluation_result = evaluate_t5_response(datapoint, t5_response)\n",
    "#     print(evaluation_result)\n",
    "#     final_score = extract_score_via_gpt(evaluation_result, client)\n",
    "#     print(final_score)\n",
    "#     print(\"---------------------------------------\")\n",
    "#     scores.append(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad7ba0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# float_scores = [float(score) for score in scores]\n",
    "# np.array(float_scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e4913fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = pd.DataFrame({\n",
    "#         \"dialogue index\": [i + 2 for i in range(len(scores))],\n",
    "#         \"test_score1\": scores\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "717c9f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"evaluation_scores.csv\")\n",
    "# adjusted_counts = exchange_counts[2:]\n",
    "\n",
    "# # Add as new column\n",
    "# df1[\"exchange_count\"] = adjusted_counts\n",
    "\n",
    "# # Define a function to categorize based on exchange count\n",
    "# def categorize_length(count):\n",
    "#     if count <= 3:\n",
    "#         return \"short\"\n",
    "#     elif 4 <= count <= 6:\n",
    "#         return \"moderate\"\n",
    "#     else:\n",
    "#         return \"long\"\n",
    "\n",
    "# # Apply it to create the new column\n",
    "# df1[\"dialogue_length\"] = df1[\"exchange_count\"].apply(categorize_length)\n",
    "# df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bdbfc6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1[\"test_score3\"] = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71fbace9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue index</th>\n",
       "      <th>test_score1</th>\n",
       "      <th>exchange_count</th>\n",
       "      <th>dialogue_length</th>\n",
       "      <th>test_score2</th>\n",
       "      <th>test_score3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>0.75</td>\n",
       "      <td>4.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0.25</td>\n",
       "      <td>6.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialogue index  test_score1  exchange_count dialogue_length  test_score2  \\\n",
       "0               2         0.00             4.0        moderate         0.00   \n",
       "1               3         0.33             4.0        moderate         0.33   \n",
       "2               4         0.40             4.0        moderate         0.40   \n",
       "3               5         0.75             4.0        moderate         0.25   \n",
       "4               6         0.25             6.0        moderate         0.25   \n",
       "\n",
       "   test_score3  \n",
       "0         0.00  \n",
       "1         0.50  \n",
       "2         0.25  \n",
       "3         0.50  \n",
       "4         0.25  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "524cb12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1.to_csv(\"evaluation_scores.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6709b76",
   "metadata": {},
   "source": [
    "fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "66064773",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(\n",
    "    data[2:],\n",
    "    test_size=20,\n",
    "    stratify=df1[\"dialogue_length\"],\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc9b2251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue index</th>\n",
       "      <th>test_score1</th>\n",
       "      <th>exchange_count</th>\n",
       "      <th>dialogue_length</th>\n",
       "      <th>test_score2</th>\n",
       "      <th>test_score3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0.33</td>\n",
       "      <td>4.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7</td>\n",
       "      <td>0.43</td>\n",
       "      <td>6.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>long</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.0</td>\n",
       "      <td>long</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>-1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dialogue index  test_score1  exchange_count dialogue_length  test_score2  \\\n",
       "1                3         0.33             4.0        moderate         0.33   \n",
       "2                4         0.40             4.0        moderate         0.40   \n",
       "5                7         0.43             6.0        moderate         0.00   \n",
       "10              12         0.00             7.0            long         0.00   \n",
       "11              13         0.00             7.0            long        -1.00   \n",
       "\n",
       "    test_score3  \n",
       "1          0.50  \n",
       "2          0.25  \n",
       "5          0.33  \n",
       "10         0.00  \n",
       "11        -1.00  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ids = [d[\"dialogue_id\"] for d in test_data]\n",
    "\n",
    "df_test = df1[df1[\"dialogue index\"].isin(test_ids)]\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ff0a965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'target'],\n",
       "    num_rows: 40\n",
       "})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs = [\n",
    "    {\n",
    "        \"input\": (\n",
    "            f\"You are a helpful system continuing a conversation with a user.\\n\\n\"\n",
    "            f\"Here is the prior dialogue:\\n{\"\\n\".join([f\"{turn['speaker']}: {turn['text']}\" for turn in item['original_context']])}\\n\\n\"\n",
    "            f\"The user now says:\\n\\\"{item['user_revision']['text']}\\\"\\n\\n\"\n",
    "            f\"Please respond meaningfully to the user's new message, based on the earlier context. \"\n",
    "            f\"Write only your reply—do not prefix with 'System:' or 'User:'.\"\n",
    "        ),\n",
    "        \"target\": item['correct_revised_response']['text']\n",
    "    }\n",
    "    for item in train_data\n",
    "]\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_list(train_pairs)\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01d99ba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'You are a helpful system continuing a conversation with a user.\\n\\nHere is the prior dialogue:\\nUser: Hi, I’m curious—what does it mean when someone says \\'receptionist fries cake\\' in a kitchen setting?\\nSystem: Great question! In culinary terms, \\'receptionist fries cake\\' can be a creative or metaphorical way of describing an action. For example, a receptionist might fries cake as part of food preparation or presentation.\\nUser: So when a receptionist friess cake, is that a standard step in a recipe?\\nSystem: Sometimes, yes. For instance, chefs may use that expression to describe specific operations in a dish, such as how a receptionist contributes to handling cake in the kitchen.\\nUser: Would that affect the flavor or texture of the cake in any way?\\nSystem: It could. The way a receptionist friess cake might influence its consistency, taste, or even plating aesthetics.\\n\\nThe user now says:\\n\"Wait a second—why is the receptionist frying cake? Isn’t that more of a kitchen staff task? Also, I noticed \\'friess\\' might be a typo. Can you revise the explanation with that in mind?\"\\n\\nPlease respond meaningfully to the user\\'s new message, based on the earlier context. Write only your reply—do not prefix with \\'System:\\' or \\'User:\\'.',\n",
       " 'target': \"Good catch on both points! The correct term should be 'fries,' not 'friess.' And you're absolutely right—frying cake is typically done by kitchen staff, not receptionists. If someone says 'receptionist fries cake,' it’s likely a humorous or metaphorical phrase, possibly describing a receptionist stepping in unexpectedly or outside their usual duties. In standard kitchen practice, tasks like frying would be handled by trained culinary staff to ensure proper technique and food safety.\"}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pairs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2d2a373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57e8f16a6d5c4fe99a1944d2822bdac3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/40 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess(example):\n",
    "    model_inputs = tokenizer(\n",
    "        example[\"input\"],\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tokenizer(\n",
    "        example[\"target\"],\n",
    "        max_length=256,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_train = train_dataset.map(preprocess, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "135a052a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05c46305347240078183fcb5ae9e7fe5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\heiha\\AppData\\Local\\Temp\\ipykernel_2732\\2272282477.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='140' max='140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [140/140 05:45, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>31.048600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>29.034900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>29.905700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>28.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>29.510200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>31.415700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>28.740900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>29.270300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>28.685500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>32.028800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>29.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>30.780700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>30.298000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>28.990300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>28.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>31.765400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>28.762500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>29.251000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>30.552600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>31.622100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>28.204100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>29.837200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>29.511900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>29.355500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>29.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>32.483200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>27.090200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>30.307300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\torch\\utils\\checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=140, training_loss=29.786612374441965, metrics={'train_runtime': 347.8474, 'train_samples_per_second': 2.3, 'train_steps_per_second': 0.402, 'total_flos': 6865045526937600.0, 'train_loss': 29.786612374441965, 'epoch': 20.0})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_enable_fp32_cpu_offload=True  # this line enables CPU offload\n",
    ")\n",
    "\n",
    "model_tune = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_name,  # e.g., \"google/flan-t5-xl\"\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "\n",
    "model_tune = prepare_model_for_kbit_training(model_tune)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],  # you can also add \"k\", \"o\" optionally\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_2_SEQ_LM\"\n",
    ")\n",
    "\n",
    "model_tune = get_peft_model(model_tune, lora_config)\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./t5-finetuned-conversations\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=6,               # fits well in 12GB\n",
    "    gradient_accumulation_steps=1,               # effective batch size = 4\n",
    "    num_train_epochs=20,                          # small dataset, more epochs helps\n",
    "    logging_steps=5,\n",
    "    save_steps=20,\n",
    "    evaluation_strategy=\"no\",\n",
    "    fp16=True,                                   # take advantage of 4070's FP16\n",
    "    save_total_limit=1\n",
    ")\n",
    "\n",
    "from transformers import Seq2SeqTrainer, DataCollatorForSeq2Seq\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model_tune,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model_tune)\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f97662f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import no_grad\n",
    "\n",
    "def generate_response_tuned(datapoint, model, tokenizer, max_new_tokens=200):\n",
    "    model.eval()  # ✅ set model to eval mode\n",
    "    dialogue_str = \"\\n\".join([f\"{turn['speaker']}: {turn['text']}\" for turn in datapoint['original_context']])\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are a helpful system continuing a conversation with a user.\\n\\n\"\n",
    "        f\"Here is the prior dialogue:\\n{dialogue_str}\\n\\n\"\n",
    "        f\"The user now says:\\n\\\"{datapoint['user_revision']['text']}\\\"\\n\\n\"\n",
    "        f\"Please respond meaningfully to the user's new message, based on the earlier context. \"\n",
    "        f\"Write only your reply—do not prefix with 'System:' or 'User:'.\"\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with no_grad():  # ✅ disable gradients during inference\n",
    "        outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e43e33f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing test datapoint 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heiha\\anaconda3\\Lib\\site-packages\\bitsandbytes\\autograd\\_functions.py:315: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: The boy is skateboarding on a red bridge, which suggests he's in an urban environment that might be iconic or picturesque due to the distinct color of the bridge.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The presence of a red bridge suggests the boy is skateboarding in an urban environment.\n",
      "- The distinct color of the red bridge implies that the environment might be iconic or picturesque.\n",
      "\n",
      "0.5\n",
      "\n",
      "The T5-generated response mirrors two points from the old logic chain of the original conversation: the boy is in an urban environment with a bridge of notable red color. However, it fails to incorporate the crucial update that the bridge is part of a major highway, which should replace pedestrian-focused elements of the initial narrative. The response is incomplete regarding the user's revision, though it does not contain any direct contradictions.\n",
      "0.5\n",
      "---------------------------------------\n",
      "Processing test datapoint 2/20\n",
      "System: The employees, with their bright shirts and smiles, likely contribute to a positive and upbeat environment, making the coffee shop an appealing spot for a peaceful yet pleasant experience. This can be especially attractive to someone who enjoys being part of a lively setting without needing direct engagement.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - Bright shirts and smiles of employees contribute to a positive and upbeat environment.\n",
      "- The positive environment makes the coffee shop appealing for a peaceful yet pleasant experience.\n",
      "- The environment is attractive to individuals who enjoy lively settings.\n",
      "- The setting allows enjoyment of a lively atmosphere without requiring direct engagement with others.\n",
      "\n",
      "0.75\n",
      "\n",
      "The T5-generated system response correctly updates by attributing the positive and upbeat atmosphere to the employees' bright clothing and describing how this setting can appeal to individuals who enjoy a lively environment. However, it fails to address the revised context, which eliminates the orange juice and reframes the focus entirely on the presence and contribution of the employees wearing orange. The explanation does not acknowledge the specific shift from a drink-based observation to the significance of staff appearance, leading to a partial alignment with the revised logic.\n",
      "0.75\n",
      "---------------------------------------\n",
      "Processing test datapoint 3/20\n",
      "System: The school is organizing a multicultural party event where different cultural traditions, including American ones, are showcased through music, food, and various performances such as dance and art displays.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The school is organizing a multicultural party event.\n",
      "- The purpose of the event is to showcase different cultural traditions.\n",
      "- American cultural traditions are included in the event.\n",
      "- Cultural traditions are showcased through music.\n",
      "- Cultural traditions are showcased through food.\n",
      "- Cultural traditions are showcased through various performances.\n",
      "- The performances include dance displays.\n",
      "- The performances include art displays.\n",
      "\n",
      "0.29\n",
      "\n",
      "The T5-generated response failed to adapt to the user's revised query focusing on culinary displays. Out of seven logic points, only two ('Cultural traditions are showcased through food' and 'American cultural traditions are included in the event') vaguely relate to the revised focus, while the rest continue to emphasize performances and music, which were noted as incorrect in the revised context.\n",
      "0.29\n",
      "---------------------------------------\n",
      "Processing test datapoint 4/20\n",
      "System: In areas with severe weather, busing or being driven by guardians typically replace walking, ensuring children’s safety during their commute.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - In areas with severe weather, walking is often not a safe option for children.\n",
      "- Busing or being driven by guardians serves as a safer alternative to walking in these conditions.\n",
      "- Ensuring children’s safety during their commute is a priority in severe weather conditions.\n",
      "\n",
      "-1\n",
      "\n",
      "The T5-generated system response simply repeats a prior logic point about severe weather conditions without addressing the user's updated information about younger children walking or cycling. It fails to reflect any of the needed logic points or adapt to the user's revised query, thus scoring -1 as it does not accomplish meaningful adaptation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (639 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "---------------------------------------\n",
      "Processing test datapoint 5/20\n",
      "System: I've updated the text to reflect this.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The system has made changes to the text.\n",
      "- The update ensures that the changes are reflected or acknowledged in the text.\n",
      "\n",
      "0.2\n",
      "\n",
      "The T5-generated response simply states that the text has been updated but does not address any specific logic point related to the user's revision about instrumental music replacing vocal performances. This response lacks meaningful content regarding the adaptation of the discussion to incorporate the user's updated context. The evaluation lacks alignment with any updated logic points.\n",
      "0.2\n",
      "---------------------------------------\n",
      "Processing test datapoint 6/20\n",
      "System: Bike boxes are a great way for cyclists to cross intersections without waiting for a light. They are designed to streamline bicycle traffic and enhance safety by separating it from vehicle flow.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - Bike boxes provide a means for cyclists to cross intersections without the need to wait for a traffic light.\n",
      "- Bike boxes are designed to streamline the flow of bicycle traffic.\n",
      "- Bike boxes enhance safety by separating bicycle traffic from vehicle flow.\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "---------------------------------------\n",
      "Processing test datapoint 7/20\n",
      "System: Yes, well-maintained gardens often attract birds, butterflies, and other small wildlife, contributing to the tranquil atmosphere. This interaction with wildlife can be one of the enjoyable aspects of spending time in the yard.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - Well-maintained gardens often attract birds, butterflies, and other small wildlife.\n",
      "- The presence of birds, butterflies, and small wildlife contributes to a tranquil atmosphere in the garden.\n",
      "- Interaction with wildlife is one of the enjoyable aspects of spending time in the garden.\n",
      "\n",
      "0.0\n",
      "\n",
      "The T5-generated response fails to adapt to the user's revision. It repeats the original logic that well-maintained gardens attract wildlife and enhance relaxation, contradicting the user's corrected perspective that people often aim to avoid attracting too much wildlife due to pests. This reflects the logic of the sample wrong response rather than aligning with the user's revision.\n",
      "0.0\n",
      "---------------------------------------\n",
      "Processing test datapoint 8/20\n",
      "System: If your car windows are made of laminated glass, you should still try to break the window.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - Laminated glass is tough to break, but attempts should still be made.\n",
      "- Attempting to break the window remains a potential option even with laminated glass.\n",
      "\n",
      "0.5\n",
      "\n",
      "The T5-generated system response acknowledges the difficulty of breaking laminated glass but still suggests attempting to break it. This partially aligns with the updated logic that such glass is tough to break, yet it fails to adequately pivot to focusing on pressure equalization and door-opening strategies required for laminated glass situations. It does not update the advice to reflect a shift from window-based to door-based escape, which is a significant aspect of the user's revised prompt.\n",
      "0.5\n",
      "---------------------------------------\n",
      "Processing test datapoint 9/20\n",
      "They would have been able to meet Aquash at the same time.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The ability to meet Aquash implies the alignment of schedules or timing.\n",
      "- Meeting simultaneously suggests a shared timeframe in which multiple parties are available.\n",
      "- The past tense \"would have been\" implies that certain conditions or intentions were in place previously.\n",
      "\n",
      "0.0\n",
      "\n",
      "The T5-generated system response fails to adapt to the user's revised context by not acknowledging the impact on the timeline or evidence implications. The response incorrectly assumes that meeting Aquash directly entails no significant changes, rather than addressing how bypassing a crucial event like stopping at Marshall's home might alter the prosecution's narrative and connections between participants and the crime. It lacks the in-depth reasoning necessary to explore the consequences of the user's hypothetical scenario.\n",
      "0.0\n",
      "---------------------------------------\n",
      "Processing test datapoint 10/20\n",
      "He made an emotional plea for her acquittal as she faces charges of involvement in the 2007 sex murder of her British housemate in Italy.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The individual is making an emotional plea, indicating their belief in her innocence or pleading for leniency.\n",
      "- The person in question is facing charges related to a murder case, specifically a sex-related murder, suggesting the severity of the crime.\n",
      "- The charges pertain to events that occurred in 2007.\n",
      "- The murder victim was a British housemate, establishing the relationship to the accused and highlighting potential international interest.\n",
      "- The incident took place in Italy, specifying the jurisdiction and potential legal complexity due to cross-border implications.\n",
      "\n",
      "0.0\n",
      "\n",
      "The T5-generated response did not adapt to the user's revised query regarding how a rational appeal would differ. It repeated original information without addressing the change in appeal from emotional to rational or its implications, making it similar to the sample wrong response.\n",
      "0.0\n",
      "---------------------------------------\n",
      "Processing test datapoint 11/20\n",
      "I think it indicates that the police are focusing on those directly involved in the Mafia’s core reconstruction plans, aiming to dismantle that network completely.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The police are concentrating their efforts on individuals who are directly involved in the Mafia’s core reconstruction plans.\n",
      "- The objective of the police is to dismantle the network associated with the Mafia's reconstruction plans completely.\n",
      "\n",
      "0.0\n",
      "\n",
      "The T5-generated system response does not adapt to the user's revised query. It repeats the initial logic that the police are focusing on those directly involved in the Mafia's core reconstruction plans, without acknowledging the user's revision that the individuals are peripheral. This failure to update the logic results in a response that reflects none of the revised considerations from the user about a possible shift in police strategy.\n",
      "0.0\n",
      "---------------------------------------\n",
      "Processing test datapoint 12/20\n",
      "System: Sorry for the confusion. I meant to say grills.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The system acknowledges there was confusion in its previous communication.\n",
      "- The system clarifies its intended message by specifying the term \"grills.\"\n",
      "\n",
      "0.5\n",
      "\n",
      "The T5-generated system response correctly addresses the typo (\"grillss\" → \"grills\"), aligning with one of the revised logic points. However, it fails to address the user's query about whether grilling watermelon is an actual cooking technique or just metaphorical, demonstrating a partial adaptation to the user's revision.\n",
      "0.5\n",
      "---------------------------------------\n",
      "Processing test datapoint 13/20\n",
      "System: I think you're right. I was referring to the sanitizer as a cleaning agent.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The system agrees with the user's perspective or statement.\n",
      "- The system identified its previous mention of sanitizer as referring to it as a cleaning agent.\n",
      "\n",
      "0.5\n",
      "\n",
      "The T5-generated response correctly acknowledges the user's re-interpretation of \"sanitizer\" as a cleaning agent, aligning with one logic point derived from the revision. However, it fails to address the error in the original interpretation of \"packages,\" neglects to offer any clarification about the original phrase being non-standard or metaphorical, nor does it discuss the separation of roles between sanitation and food handling as described in the correct revised logic. Thus, it only partially adapts to the user's revised query.\n",
      "0.5\n",
      "---------------------------------------\n",
      "Processing test datapoint 14/20\n",
      "System: Sure. In a front-of-house context, 'greets glass' can refer to the receptionist preparing glasses for guests.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The term 'greets glass' relates to front-of-house operations.\n",
      "- 'Greets glass' can imply the receptionist is involved.\n",
      "- The action refers to the preparation of glasses for guests.\n",
      "\n",
      "0.6667\n",
      "\n",
      "The T5-generated system response mostly reflects the user's revised query by correctly identifying that \"greets glass\" relates to the preparation of glasses for guests in a front-of-house context, aligning well with the revised logic of the situation. However, it lacks depth in elaborating on how this action contributes to overall dining presentation, thereby missing one point related to service aesthetics and detail orientation described in the correct response.\n",
      "0.6667\n",
      "---------------------------------------\n",
      "Processing test datapoint 15/20\n",
      "System: Yes, that's correct. A restaurant owner serves shrimp as part of a recipe, or as part of a creative or metaphorical way of describing an action.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - A restaurant owner serves shrimp as part of a recipe.\n",
      "- A restaurant owner might use shrimp in a creative way to describe an action.\n",
      "\n",
      "0.0\n",
      "\n",
      "The T5-generated response fails to adapt to the user's revised query. It repeats the original context that the restaurant owner serves shrimp as part of a recipe or creatively, without addressing the correction that the owner typically enables the process through other means rather than directly serving the shrimp. This maintains the original logic without any meaningful adaptation to the user's requested revision.\n",
      "0.0\n",
      "---------------------------------------\n",
      "Processing test datapoint 16/20\n",
      "System: I have you at Jinling Noodle bar then, reference is OCNH3JJ . Anything else you need?\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The system acknowledges the reservation for the user at Jinling Noodle Bar.\n",
      "- A reference code, OCNH3JJ, is provided for the reservation.\n",
      "- The system inquires if the user requires any additional assistance or information.\n",
      "\n",
      "0.0\n",
      "\n",
      "The T5-generated response repeats the initial action of confirming a reservation at a Chinese restaurant without acknowledging the user's revised request for a Japanese restaurant. It fails to update the booking or provide a new reference, reflecting none of the correct logical updates needed for the user's new intent.\n",
      "0.0\n",
      "---------------------------------------\n",
      "Processing test datapoint 17/20\n",
      "System: I have updated the booking to a grey Audi. The contact number is 07597249308.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The booking has been amended to now include a grey Audi.\n",
      "- A contact number associated with the booking is provided: 07597249308.\n",
      "\n",
      "0.5\n",
      "\n",
      "The T5-generated response fails to update the destination to Cambridge station, which is a critical component of the user's revised query. While it does mention that the booking involves a grey Audi with the same contact information, it does not address changes to the destination, which is pivotal to adapting to the user's request. Thus, it only partially reflects the updated logic, similar to the example of incorporating revised information without correct adjustments.\n",
      "0.5\n",
      "---------------------------------------\n",
      "Processing test datapoint 18/20\n",
      "System: I have a restaurant called Cote that serves Italian food. It is expensive. Would you like me to book you a reservation?\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The restaurant is named Cote.\n",
      "- Cote serves Italian food.\n",
      "- Cote is classified as an expensive restaurant.\n",
      "- The system offers to make a reservation at Cote.\n",
      "\n",
      "0.25\n",
      "\n",
      "The T5-generated system failed to adapt to the user's revised cuisine preference properly. It incorrectly maintains the French restaurant, Cote, while claiming it serves Italian food, which creates a contradiction in the response. This reflects only one logically correct update (offering a reservation for the right cuisine type) out of four key logic points.\n",
      "0.25\n",
      "---------------------------------------\n",
      "Processing test datapoint 19/20\n",
      "System: Cocum does not have vegetarian options. Is there another restaurant you would like to try?\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - Cocum does not offer vegetarian options.\n",
      "- The system infers that the user might want to try another restaurant.\n",
      "- The system prompts the user to consider an alternative restaurant choice.\n",
      "\n",
      "0.33\n",
      "\n",
      "The T5-generated system response includes three logic points, but only one of them is logically aligned with the user's revised request. It acknowledges that Cocum may not meet the new dietary requirement for vegetarian options. However, the system response incorrectly states that Cocum does not have vegetarian options without verifying this, and fails to suggest an alternative restaurant that meets the user's criterion.\n",
      "0.33\n",
      "---------------------------------------\n",
      "Processing test datapoint 20/20\n",
      "System: Okay, I have updated the booking. The taxi will arrive at La Raza at 10:15.\n",
      "checker2\n",
      "checker1\n",
      "extracted logic points: - The booking has been updated.\n",
      "- A taxi is scheduled to arrive.\n",
      "- The location for the taxi's arrival is La Raza.\n",
      "- The scheduled arrival time for the taxi is 10:15.\n",
      "\n",
      "0.25\n",
      "\n",
      "The T5-generated response incorrectly interprets the user's revised requirement by maintaining the arrival time at 10:15 instead of before 10:15. While it correctly identifies the location as La Raza, it fails to adjust the time logic to reflect the need to arrive beforehand. Therefore, only one out of four key points aligns with the user's revised request.\n",
      "0.25\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tuned_scores = []\n",
    "tuned_responses = []\n",
    "\n",
    "# Filter to only include datapoints whose dialogue_id is in test_ids\n",
    "test_datapoints = [d for d in data if d[\"dialogue_id\"] in test_ids]\n",
    "\n",
    "for i, datapoint in enumerate(test_datapoints):\n",
    "    print(f\"Processing test datapoint {i+1}/{len(test_datapoints)}\")\n",
    "    \n",
    "    tuned_t5_response = generate_response_tuned(datapoint, model_tune, tokenizer)  # ← using your fine-tuned model now\n",
    "    tuned_responses.append(tuned_t5_response)\n",
    "    print(tuned_t5_response)\n",
    "    \n",
    "    evaluation_result = evaluate_t5_response(datapoint, tuned_t5_response)\n",
    "    print(evaluation_result)\n",
    "    \n",
    "    final_score = extract_score_via_gpt(evaluation_result, client)\n",
    "    print(final_score)\n",
    "    print(\"---------------------------------------\")\n",
    "    \n",
    "    tuned_scores.append(final_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d84ad59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original scores: 0.2675\n",
      "Tuned scores: 0.211835\n"
     ]
    }
   ],
   "source": [
    "original_scores = df_test[\"test_score1\"].mean()\n",
    "print(f\"Original scores: {original_scores}\")\n",
    "float_tuned_scores = [float(score) for score in tuned_scores]\n",
    "tuned_scores_mean = np.array(float_tuned_scores).mean()\n",
    "print(f\"Tuned scores: {tuned_scores_mean}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b63b11b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue index</th>\n",
       "      <th>test_score1</th>\n",
       "      <th>test_score2</th>\n",
       "      <th>tuned_responses</th>\n",
       "      <th>test_score3</th>\n",
       "      <th>test_score4</th>\n",
       "      <th>test_score6</th>\n",
       "      <th>tuned_responses6</th>\n",
       "      <th>test_score7</th>\n",
       "      <th>tuned_responses7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: The boy is skateboarding on a red brid...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333</td>\n",
       "      <td>System: The boy is skateboarding on a red brid...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>System: The boy is skateboarding on a red brid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>0.60</td>\n",
       "      <td>0.60</td>\n",
       "      <td>System: The employees, with their bright shirt...</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.750</td>\n",
       "      <td>System: The employees, with their bright shirt...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>System: The employees, with their bright shirt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: The school is organizing a multicultur...</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.330</td>\n",
       "      <td>System: The school is organizing a multicultur...</td>\n",
       "      <td>0.43</td>\n",
       "      <td>System: The school is organizing a multicultur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: In areas with severe weather, busing o...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>System: In areas with severe weather, busing o...</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>System: In areas with severe weather, busing o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: I've updated the text to reflect this.</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>System: I've updated the text to reflect this.</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: I've updated the text to reflect this.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: Bike boxes are a great way for cyclist...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>System: Bike boxes are a great way for cyclist...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>System: Bike boxes are a great way for cyclist...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>19</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: Yes, well-maintained gardens often att...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>System: Yes, well-maintained gardens often att...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: Yes, well-maintained gardens often att...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>21</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>System: If your car windows are made of lamina...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.000</td>\n",
       "      <td>System: If your car windows are made of lamina...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>System: If your car windows are made of lamina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>24</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>They would have been able to meet Aquash at th...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>They would have been able to meet Aquash at th...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>They would have been able to meet Aquash at th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>26</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>He made an emotional plea for her acquittal as...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.250</td>\n",
       "      <td>He made an emotional plea for her acquittal as...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>He made an emotional plea for her acquittal as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>I think it indicates that the police are focus...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>I think it indicates that the police are focus...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>I think it indicates that the police are focus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>35</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>System: Sorry for the confusion. I meant to sa...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.250</td>\n",
       "      <td>System: Sorry for the confusion. I meant to sa...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: Sorry for the confusion. I meant to sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>38</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: I think you're right. I was referring ...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>System: I think you're right. I was referring ...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: I think you're right. I was referring ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>42</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>System: Sure. In a front-of-house context, 'gr...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.670</td>\n",
       "      <td>System: Sure. In a front-of-house context, 'gr...</td>\n",
       "      <td>0.75</td>\n",
       "      <td>System: Sure. In a front-of-house context, 'gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: Yes, that's correct. A restaurant owne...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>System: Yes, that's correct. A restaurant owne...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: Yes, that's correct. A restaurant owne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>47</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: I have you at Jinling Noodle bar then,...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>System: I have you at Jinling Noodle bar then,...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>System: I have you at Jinling Noodle bar then,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>48</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: I have updated the booking to a grey A...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>System: I have updated the booking to a grey A...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: I have updated the booking to a grey A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>49</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.25</td>\n",
       "      <td>System: I have a restaurant called Cote that s...</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.250</td>\n",
       "      <td>System: I have a restaurant called Cote that s...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>System: I have a restaurant called Cote that s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>58</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: Cocum does not have vegetarian options...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>System: Cocum does not have vegetarian options...</td>\n",
       "      <td>0.50</td>\n",
       "      <td>System: Cocum does not have vegetarian options...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>59</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.67</td>\n",
       "      <td>System: Okay, I have updated the booking. The ...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.500</td>\n",
       "      <td>System: Okay, I have updated the booking. The ...</td>\n",
       "      <td>0.33</td>\n",
       "      <td>System: Okay, I have updated the booking. The ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    dialogue index  test_score1  test_score2  \\\n",
       "0                3         0.50         0.50   \n",
       "1                4         0.60         0.60   \n",
       "2                7         0.50         0.50   \n",
       "3               12         0.00         0.00   \n",
       "4               13         0.00         0.00   \n",
       "5               14         0.00         0.00   \n",
       "6               19         0.00         0.00   \n",
       "7               21         0.25         0.25   \n",
       "8               24         0.25         0.25   \n",
       "9               26         0.00         0.00   \n",
       "10              28         0.00         0.00   \n",
       "11              35         0.33         0.33   \n",
       "12              38         0.50         0.50   \n",
       "13              42         0.67         0.67   \n",
       "14              43         0.00         0.00   \n",
       "15              47         0.00         0.00   \n",
       "16              48         0.50         0.50   \n",
       "17              49         0.25         0.25   \n",
       "18              58         0.50         0.50   \n",
       "19              59         0.67         0.67   \n",
       "\n",
       "                                      tuned_responses  test_score3  \\\n",
       "0   System: The boy is skateboarding on a red brid...         0.50   \n",
       "1   System: The employees, with their bright shirt...         0.40   \n",
       "2   System: The school is organizing a multicultur...         0.43   \n",
       "3   System: In areas with severe weather, busing o...         0.00   \n",
       "4      System: I've updated the text to reflect this.         0.20   \n",
       "5   System: Bike boxes are a great way for cyclist...         0.00   \n",
       "6   System: Yes, well-maintained gardens often att...         0.00   \n",
       "7   System: If your car windows are made of lamina...         0.33   \n",
       "8   They would have been able to meet Aquash at th...         0.00   \n",
       "9   He made an emotional plea for her acquittal as...         0.00   \n",
       "10  I think it indicates that the police are focus...         0.00   \n",
       "11  System: Sorry for the confusion. I meant to sa...         0.33   \n",
       "12  System: I think you're right. I was referring ...         0.50   \n",
       "13  System: Sure. In a front-of-house context, 'gr...         0.75   \n",
       "14  System: Yes, that's correct. A restaurant owne...         0.00   \n",
       "15  System: I have you at Jinling Noodle bar then,...         0.00   \n",
       "16  System: I have updated the booking to a grey A...         0.50   \n",
       "17  System: I have a restaurant called Cote that s...         0.25   \n",
       "18  System: Cocum does not have vegetarian options...         0.33   \n",
       "19  System: Okay, I have updated the booking. The ...         0.33   \n",
       "\n",
       "    test_score4  test_score6  \\\n",
       "0          0.50        0.333   \n",
       "1          0.40        0.750   \n",
       "2          0.40        0.330   \n",
       "3          0.00        0.000   \n",
       "4          0.00        0.000   \n",
       "5          0.00        0.000   \n",
       "6          0.00        0.000   \n",
       "7          0.25        0.000   \n",
       "8          0.00        0.000   \n",
       "9          0.17        0.250   \n",
       "10         0.00        0.000   \n",
       "11         0.33        0.250   \n",
       "12         0.50        0.500   \n",
       "13         0.67        0.670   \n",
       "14         0.00        0.000   \n",
       "15         0.00        0.000   \n",
       "16         0.50        0.500   \n",
       "17         0.40        0.250   \n",
       "18         0.50        0.500   \n",
       "19         0.33        0.500   \n",
       "\n",
       "                                     tuned_responses6  test_score7  \\\n",
       "0   System: The boy is skateboarding on a red brid...         0.40   \n",
       "1   System: The employees, with their bright shirt...         0.25   \n",
       "2   System: The school is organizing a multicultur...         0.43   \n",
       "3   System: In areas with severe weather, busing o...        -1.00   \n",
       "4      System: I've updated the text to reflect this.         0.00   \n",
       "5   System: Bike boxes are a great way for cyclist...         0.25   \n",
       "6   System: Yes, well-maintained gardens often att...         0.00   \n",
       "7   System: If your car windows are made of lamina...         0.33   \n",
       "8   They would have been able to meet Aquash at th...         0.00   \n",
       "9   He made an emotional plea for her acquittal as...         0.00   \n",
       "10  I think it indicates that the police are focus...         0.00   \n",
       "11  System: Sorry for the confusion. I meant to sa...         0.50   \n",
       "12  System: I think you're right. I was referring ...         0.50   \n",
       "13  System: Sure. In a front-of-house context, 'gr...         0.75   \n",
       "14  System: Yes, that's correct. A restaurant owne...         0.50   \n",
       "15  System: I have you at Jinling Noodle bar then,...         0.00   \n",
       "16  System: I have updated the booking to a grey A...         0.50   \n",
       "17  System: I have a restaurant called Cote that s...         0.33   \n",
       "18  System: Cocum does not have vegetarian options...         0.50   \n",
       "19  System: Okay, I have updated the booking. The ...         0.33   \n",
       "\n",
       "                                     tuned_responses7  \n",
       "0   System: The boy is skateboarding on a red brid...  \n",
       "1   System: The employees, with their bright shirt...  \n",
       "2   System: The school is organizing a multicultur...  \n",
       "3   System: In areas with severe weather, busing o...  \n",
       "4      System: I've updated the text to reflect this.  \n",
       "5   System: Bike boxes are a great way for cyclist...  \n",
       "6   System: Yes, well-maintained gardens often att...  \n",
       "7   System: If your car windows are made of lamina...  \n",
       "8   They would have been able to meet Aquash at th...  \n",
       "9   He made an emotional plea for her acquittal as...  \n",
       "10  I think it indicates that the police are focus...  \n",
       "11  System: Sorry for the confusion. I meant to sa...  \n",
       "12  System: I think you're right. I was referring ...  \n",
       "13  System: Sure. In a front-of-house context, 'gr...  \n",
       "14  System: Yes, that's correct. A restaurant owne...  \n",
       "15  System: I have you at Jinling Noodle bar then,...  \n",
       "16  System: I have updated the booking to a grey A...  \n",
       "17  System: I have a restaurant called Cote that s...  \n",
       "18  System: Cocum does not have vegetarian options...  \n",
       "19  System: Okay, I have updated the booking. The ...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df2 = pd.DataFrame({\n",
    "#     \"dialogue index\": [item[\"dialogue_id\"] for item in test_datapoints],\n",
    "#     \"test_score1\": tuned_scores\n",
    "# })\n",
    "df3 = pd.read_csv(\"tuned_evaluation_scores.csv\")\n",
    "# df3[\"test_score7\"] = tuned_scores\n",
    "# df3[\"tuned_responses7\"] = tuned_responses\n",
    "# df3.to_csv(\"tuned_evaluation_scores.csv\", index=False)\n",
    "# df2[df2[\"test_score1\"]!= \"-1\"][\"test_score1\"].astype(float).mean()\n",
    "df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cdff6e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
